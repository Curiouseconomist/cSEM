<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Postestimation: Assessing a model • cSEM</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha256-916EbMg70RQy9LHiGkXzG8hSg9EdNy97GazNG/aiY1w=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha256-U5ZEeKfGNOja007MMD3YBI0A3OSZOQbeG6z2f2Y0hu8=" crossorigin="anonymous"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.7.1/css/all.min.css" integrity="sha256-nAmazAk6vS34Xqo0BSrTb+abbtFlgsFK7NKSi6o7Y78=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.7.1/css/v4-shims.min.css" integrity="sha256-6qHlizsOWFskGlwVOKuns+D1nB6ssZrHQrNj1wGplHc=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" integrity="sha256-FiZwavyI2V6+EXO1U+xzLG3IKldpiTFf3153ea9zikQ=" crossorigin="anonymous"></script><!-- sticky kit --><script src="https://cdnjs.cloudflare.com/ajax/libs/sticky-kit/1.1.3/sticky-kit.min.js" integrity="sha256-c4Rlo1ZozqTPE2RLuvbusY3+SU1pQaJC0TjuhygMipw=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Postestimation: Assessing a model">
<meta property="og:description" content="">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">cSEM</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.0.0.9000</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fas fa fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../articles/cSEM.html">Get started</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/Using-assess.html">Postestimation: Assessing a model</a>
    </li>
    <li>
      <a href="../articles/Using-summarize.html">Postestimation: Summarizing results</a>
    </li>
    <li>
      <a href="../articles/Using-test.html">Postestimation: Testing a model</a>
    </li>
    <li>
      <a href="../articles/Using-verify.html">Postestimation: Verifying results</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/M-E-Rademaker/cSEM">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1>Postestimation: Assessing a model</h1>
            <h3 class="subtitle">Using the <code><a href="../reference/assess.html">assess()</a></code> function</h3>
            
            <h4 class="date">Last edited: 2019-04-24</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/M-E-Rademaker/cSEM/blob/master/vignettes/Using-assess.Rmd"><code>vignettes/Using-assess.Rmd</code></a></small>
      <div class="hidden name"><code>Using-assess.Rmd</code></div>

    </div>

    
    
<p>WARNING: this document is work in progress and may still contain mistakes!</p>
<div id="introduction" class="section level1">
<h1 class="hasAnchor">
<a href="#introduction" class="anchor"></a>Introduction</h1>
<p>As indicated by the name, <code><a href="../reference/assess.html">assess()</a></code> is used to assess a model estimated using the <code><a href="../reference/csem.html">csem()</a></code> function.</p>
<p>In <code>cSEM</code> <em>model assessement</em> is considered to be any task that in some way or another seeks to assess the quality of the estimated model <em>without conducting</em> <em>a statistical test</em> (tests are covered by the <code>test_*</code> family of functions). Quality in this case is taken to be a catch-all term for all common aspects of model assessment. This mainly comprises fit indices, reliability estimates, common validity assessment criteria and other related quality measures/indices that do not rely on a formal test procedure. Hereinafter, we will refer to a generic (fit) index, criteria or assessment measure as a <strong>quality measure</strong>. Currently the following quality measures are implemented:</p>
<ul>
<li>Convergent and discriminant validity assessment:
<ul>
<li>The <strong>average variance extracted</strong> (AVE) (the basis for the Fornell-Larcker criteria (FSE))</li>
<li>The <strong>heterotrait-monotrait ratio of correlations</strong> (HTMT)</li>
</ul>
</li>
<li>
<strong>Congeneric reliability</strong> (<span class="math inline">\(\rho_C\)</span>), also known as e.g.: composite reliability, construct reliability, (unidimensional) omega, Jöreskog’s <span class="math inline">\(\rho\)</span>, <span class="math inline">\(\rho_A\)</span>, or <span class="math inline">\(\rho_B\)</span>.</li>
<li>
<p><strong>Tau-equivalent reliability</strong> (<span class="math inline">\(\rho_T\)</span>), also known as e.g.: Cronbach alpha, alpha, <span class="math inline">\(\alpha\)</span>, coefficient alpha, Guttman’s <span class="math inline">\(\lambda_3\)</span>, KR-20.</p>
A third reliability measure is <strong>Parallel reliability</strong> (<span class="math inline">\(\rho_P\)</span>), also known as e.g.: split-half reliability, Spearman-Brown formulae/prophecy, standarized alpha. In <code>cSEM</code> parallel reliability is always identical to tau-equivalent reliability as indicators are always standardized. Hence, only <span class="math inline">\(\rho_T\)</span> is reported.</li>
<li>Distance measures
<ul>
<li>The <strong>standardized root mean square residual</strong> (SRMR)</li>
<li>The <strong>geodesic distance</strong> (DG)</li>
<li>The <strong>squared Euclidian distance</strong> (DL)</li>
<li>The <strong>maximum-likelihood distance</strong> (DML)</li>
</ul>
</li>
<li>The <strong>variance inflation factors</strong> (VIF)</li>
<li>The (adjusted) coefficient of determination (<span class="math inline">\(R^2\)</span> and <span class="math inline">\(R^2_{adj}\)</span>)</li>
<li>A measure of the <strong>effect size</strong>
</li>
<li><p><strong>Redundancy analysis</strong> (RA)</p></li>
</ul>
<p>For implementation details see the <a href="#methods">Methods &amp; Formulae</a> section.</p>
<div id="syntax-options" class="section level2">
<h2 class="hasAnchor">
<a href="#syntax-options" class="anchor"></a>Syntax &amp; Options</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="kw"><a href="../reference/assess.html">assess</a></span>(</a>
<a class="sourceLine" id="cb1-2" data-line-number="2">  <span class="dt">.object              =</span> <span class="ot">NULL</span>,</a>
<a class="sourceLine" id="cb1-3" data-line-number="3">  <span class="dt">.what                =</span> <span class="kw"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="st">"all"</span>, <span class="st">"ave"</span>, <span class="st">"rhoP"</span>, <span class="st">"rhoT"</span>, <span class="st">"rhoC"</span>, <span class="st">"HTMT"</span>, </a>
<a class="sourceLine" id="cb1-4" data-line-number="4">                           <span class="st">"SRMR"</span>, <span class="st">"RMSEA"</span>)</a>
<a class="sourceLine" id="cb1-5" data-line-number="5">  <span class="dt">.only_common_factors =</span> <span class="ot">TRUE</span></a>
<a class="sourceLine" id="cb1-6" data-line-number="6">)</a></code></pre></div>
<dl>
<dt><code>.object</code></dt>
<dd>
<p>An object of class <code>cSEMResults</code> resulting from a call to <code><a href="../reference/csem.html">csem()</a></code>. Note that, technically, a call to <code><a href="../reference/csem.html">csem()</a></code> results in an object with at least two class attributes. The first class attribute is always <code>cSEMResults</code>. The second depends on the estimated model and/or the type of data provided. Technically, method dispatch is based on the second class attribute, however, practically this does not affect how the function is used. See the <a href="#details">Details</a> section below.</p>
</dd>
<dt><code>.what</code></dt>
<dd>
<p>A character string or a vector of character strings naming the quality measure to compute. By default all quality measures are computed (<code>"all"</code>).</p>
</dd>
<dt><code>.only_common_factors</code></dt>
<dd>
<p>Logical. Should quality measures be calculated for constructs modeled as common factors only? Defaults to <code>TRUE</code>. See the <a href="#details">Details</a> section below.</p>
</dd>
</dl>
</div>
<div id="details" class="section level2">
<h2 class="hasAnchor">
<a href="#details" class="anchor"></a>Details</h2>
<p>In line with all of <code>cSEM</code>’s postestimation functions, <code><a href="../reference/assess.html">assess()</a></code> is a generic function with methods for objects of class <code>cSEMResults_default</code>, <code>cSEMResults_multi</code>, <code>cSEMResults_2ndorder</code>. In <code>cSEM</code> every <code>cSEMResults_*</code> object must also have class <code>cSEMResults</code> for internal reasons. When using one of the major postestimation functions, method dispatch is therefore technically done on one of the <code>cSEMResults_*</code> class attributes, ignoring the <code>cSEMResults</code> class attribute. As long as <code><a href="../reference/assess.html">assess()</a></code> is used directly method dispatch is not of any practical concern to the end-user. The difference, however, becomes important if a user seeks to directly invoke an internal function which is called by <code><a href="../reference/assess.html">assess()</a></code> (e.g., <code><a href="../reference/calculateAVE.html">calculateAVE()</a></code> or <code><a href="../reference/calculateHTMT.html">calculateHTMT()</a></code>). In this case only objects of class <code>cSEMResults_default</code> are accepted as this ensures a specific structure. Therefore, it is important to remember that <em>internal functions are generally <strong>not</strong> generic.</em></p>
<p>Some assessment measures are inherently tied to the common factor model. It is therefore unclear how to interpret their results in the context of a composite model. Consequently, their computation is suppressed by default for constructs modeled as common factors. Currently, this applies to the following quality measures:</p>
<ul>
<li>AVE and validity assessment based theron (i.e., the Fornell-Larcker criterion)</li>
<li>HTMT and validity assessment based theron</li>
<li>All reliability measures (congeneric, tau-equivalen, and parallel)</li>
</ul>
<p>It is possible to force computation of all quality measures for constructs modeled as composites, however, we explicitly warn to interpret results with caution, as they may not even have a conceptual meaning.</p>
<p>All quality measures assume that the estimated loadings, construct correlations and path coefficients involved in the computation of a specific qualitiy measure are consistent estimates for their theoretical population counterpart. If the user deliberately chooses an approach that yields inconsistent estimates (by setting <code>.disattenuate = FALSE</code> in <code><a href="../reference/csem.html">csem()</a></code> when the model contains constructs modeled as common factors) <code><a href="../reference/assess.html">assess()</a></code> will still estimate all quantities, however, a warning is displayed as quantities such as the AVE or the congeneric reliability <span class="math inline">\(\rho_C\)</span> inherit inconsistency making their interpretation at the very least dubious.</p>
</div>
</div>
<div id="usage-examples" class="section level1">
<h1 class="hasAnchor">
<a href="#usage-examples" class="anchor"></a>Usage &amp; Examples</h1>
<div id="example1" class="section level2">
<h2 class="hasAnchor">
<a href="#example1" class="anchor"></a>Example 1</h2>
</div>
<div id="example2" class="section level2">
<h2 class="hasAnchor">
<a href="#example2" class="anchor"></a>Example 2</h2>
</div>
<div id="example3" class="section level2">
<h2 class="hasAnchor">
<a href="#example3" class="anchor"></a>Example 3</h2>
</div>
</div>
<div id="methods" class="section level1">
<h1 class="hasAnchor">
<a href="#methods" class="anchor"></a>Methods &amp; Formulae</h1>
<p>This section provides technical details and relevant formulae.</p>
<p>Define the general measurement model as: <span class="math display">\[ x_{kj} = \eta_{kj} + \varepsilon_{kj} = \lambda_{kj}\eta_j +  \varepsilon_{kj}\quad\text{for}\quad k = 1, \dots, K_j\quad\text{and}\quad j = 1, \dots, J\]</span></p>
<p>Call <span class="math inline">\(\eta_{jk} = \lambda_{jk}\eta_j\)</span> the true score and <span class="math inline">\(\eta_j\)</span> the underlying latent variable supposed to be the common factor or cause for the <span class="math inline">\(K_j\)</span> indicators conntected to latent variable <span class="math inline">\(\eta_j\)</span>. Call <span class="math inline">\(\lambda_{kj}\)</span> the loading or direct effect of the latent variable on its indicator. Let <span class="math inline">\(x_{kj}\)</span> be an indicator, <span class="math inline">\(\varepsilon_{kj}\)</span> be a measurement error and<br><span class="math display">\[\hat{\eta}_j = \sum^{K_j}_{k = 1} w_{kj} x_{kj} = \sum^{K_j}_{k = 1} w_{kj} \eta_{kj} + \sum^{K_j}_{k = 1} w_{kj} \varepsilon_{kj}
= \bar\eta_{kj} + \bar\varepsilon_{kj}\]</span> be a proxy/test score/composite/stand-in for <span class="math inline">\(\eta_j\)</span>, where <span class="math inline">\(w_{kj}\)</span> is a weight and <span class="math inline">\(\bar\eta_k\)</span> a population proxy, i.e., a weight sum of true scores. We will refer to <span class="math inline">\(\hat\eta\)</span> as “proxy” below as it best reflects its intended use.</p>
<p>Assume that <span class="math inline">\(E(\varepsilon_{kj}) = E(\eta_j) = Cov(\eta_j, \varepsilon_{kj}) = 0\)</span>. Further assume <span class="math inline">\(Cov(\varepsilon_{kj}, \varepsilon_{lj})=0\)</span> for <span class="math inline">\(k \neq l\)</span> and that <span class="math inline">\(Var(\eta_j) = E(\eta^2_j) = 1\)</span> to determine the scale.</p>
<p>Treatment below is done for a generic true score <span class="math inline">\(\eta_{jk}\)</span>. For the sake of clarity the index <span class="math inline">\(j\)</span> is therefore dropped unless it is necessary to avoid confusion.</p>
<p>Note that classical treatment of reliability and other measures below is typically done assuming the proxy <span class="math inline">\(\hat\eta_j\)</span> is a simple sum score which implies that all weighs are set to one. The treatmeant below is more general since <span class="math inline">\(\hat{\eta}_j\)</span> is allowed to be <em>any</em> weighted sum of related indicators. Readers familiar with the “classical treatment” may simply set weights to one (unit weights) to “translate” results to known formulae.</p>
<p>Based on the assumptions and definitions above the following quantities necessarily follow:</p>
<p><span class="math display">\[\begin{align}
Cov(x_k, \eta) &amp;= \lambda_k \\
Cor(x_k, \eta) &amp;= \rho_{x_k, \eta} = \frac{\lambda_k}{\sqrt{Var(x_k)}} \\
Var(\eta_k) &amp;= \lambda^2_k \\
Var(x_k)    &amp;= \lambda^2_k + Var(\varepsilon_k) \\
Cov(\eta_k, \eta_l) &amp;= E(\eta_k\eta_l) = \lambda_k\lambda_lE(\eta^2) = \lambda_k\lambda_l \\
Cov(x_k, x_j) &amp;= \lambda_k\lambda_lE(\eta^2) + \lambda_kE(\eta\varepsilon_k) + \lambda_lE(\eta\varepsilon_l) + E(\varepsilon_k\varepsilon_l) = \lambda_k\lambda_l \\
Var(\hat\eta) &amp;= \sum w_k^2(\lambda^2_k + Var(e_k)) + 2\sum_{k &lt; l} w_k w_l \lambda_k\lambda_j = \mathbf{w}'\mathbf{\Sigma}\mathbf{w} \\
Var(\bar\eta) &amp;= \sum w_k^2\lambda^2_k + 2\sum_{k &lt; l} w_k w_l \lambda_k\lambda_j =(\sum w_k\lambda_k)^2 = (\mathbf{w}'\mathbf{\lambda})^2 \\
Cov(\eta, \hat\eta) &amp;= E(\sum w_k \lambda_k \eta^2) = \sum w_k\lambda_k = \mathbf{w}'\mathbf{\lambda}= \sqrt{Var(\bar\eta)}
\end{align}\]</span> where <span class="math inline">\(\mathbf\Sigma\)</span> is the indicator variance covariance matrix implied by the measurement model:</p>
<p><span class="math display">\[
\mathbf\Sigma = \begin{pmatrix}
\lambda^2_1 + Var(\varepsilon_1) &amp; \lambda_1\lambda_2  &amp; \dots &amp; \lambda_1\lambda_K \\
\lambda_2\lambda_ 1 &amp; \lambda^2_2 + Var(\varepsilon_2) &amp; \dots &amp; \lambda_2\lambda_K \\
 \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\lambda_{K}\lambda_1 &amp; \lambda_K\lambda_2 &amp;\dots &amp;\lambda^2_K + Var(\varepsilon_K)
\end{pmatrix}
\]</span></p>
<p>In <code>cSEM</code> indicators are always standardized (i.e., <span class="math inline">\(Var(x_k) = 1\)</span>) and weights are always appropriately scaled such that the variance of <span class="math inline">\(\hat\eta\)</span> is equal to one! For most of the formulae below this implies a significant simplification, however, for ease of comparison to extant literature formulae will be derived and written in general form. The “simplified form” or “cSEM form” using <span class="math inline">\(Var(x_k) = Var(\hat\eta) = 1\)</span> will be given at the end of each paragraph.</p>
<div id="average-variance-extracted-ave" class="section level2">
<h2 class="hasAnchor">
<a href="#average-variance-extracted-ave" class="anchor"></a>Average variance extracted (AVE)</h2>
<div id="definition" class="section level3">
<h3 class="hasAnchor">
<a href="#definition" class="anchor"></a>Definition</h3>
<p>Several definitions exist. For ease of comparison to extant literature the most common definitions are given below:</p>
<ul>
<li>The AVE is the share of the total indicator variance that is captured by the proxy.</li>
<li>The AVE is the ratio of the sum of the true score variances (explained variation) relative to the sum of the total indicator variances (total variation).</li>
<li>The AVE gauges how much of the variation in the indicators is due to the assumed latent variable. Consequently, the share of unexplained, i.e. error variation is 1 - AVE.</li>
<li>Since the <span class="math inline">\(R^2_k\)</span> from a regression of <span class="math inline">\(x_k\)</span> on <span class="math inline">\(\eta\)</span> is equal to the share of the explained variation relative to the share of total variation, the AVE is a sum over all <span class="math inline">\(R^2_k\)</span>.</li>
</ul>
<p>It is important to stress that, although different in wording, all definitions are synonymous!</p>
</div>
<div id="formula" class="section level3">
<h3 class="hasAnchor">
<a href="#formula" class="anchor"></a>Formula</h3>
<p>Using the results and notation derived above, the AVE for a generic construct is: <span class="math display">\[ AVE = \frac{\sum Var(\eta_k)}{\sum Var(x_k)} = \frac{\sum\lambda^2_k}{\sum(\lambda^2_k + Var(\varepsilon_k))}\]</span> If <span class="math inline">\(x_k\)</span> is standardized the denominator reduces to <span class="math inline">\(K\)</span> and the AVE is <span class="math display">\[ AVE = \frac{1}{K}\sum \lambda^2_k = \frac{1}{K}\sum \rho_{x_k, \eta}^2\]</span> As an important consequence, the AVE is closely tied the communality. <strong>Indicator communality</strong> (<span class="math inline">\(COM_k\)</span>) is definied as the square of the standardized loading of the <span class="math inline">\(k\)</span>’th indicator (<span class="math inline">\(\lambda^2\)</span>). <strong>Construct (total) communality</strong> (<span class="math inline">\(COM = \varnothing COM_k\)</span>) is definied as the mean over all indicator communalities. Since indicators, proxies and subsequently loadings are always standardized, the squared loading is simply the squared correlation between the indicator and its related population proxy. The AVE is also directly related to another frequently used term, <strong>indicator reliability</strong>, defined as the squared correlation between an indicator <span class="math inline">\(k\)</span> and its related population proxy (see section <a href="#reliability">Reliability</a> below), which is again simply <span class="math inline">\(\lambda^2\)</span>. Therefore in <code>cSEM</code> we always have:</p>
<p><span class="math display">\[ AVE = COM = \frac{1}{K}\sum COM_k = \frac{1}{K}\sum \text{Indicator reliability}_k = \frac{1}{K}\sum R^2_k \]</span></p>
</div>
<div id="implementation" class="section level3">
<h3 class="hasAnchor">
<a href="#implementation" class="anchor"></a>Implementation</h3>
<p>The function is implemented as: <code><a href="../reference/calculateAVE.html">calculateAVE()</a></code>. See <code><a href="../reference/calculateAVE.html">?calculateAVE</a></code> for a list of arguments and their description.</p>
</div>
<div id="see-also" class="section level3">
<h3 class="hasAnchor">
<a href="#see-also" class="anchor"></a>See also</h3>
<p>The AVE is the basis for the Fornell-Larcker criterion. See (TODO).</p>
</div>
</div>
<div id="reliability" class="section level2">
<h2 class="hasAnchor">
<a href="#reliability" class="anchor"></a>Reliability</h2>
<div id="definition-1" class="section level3">
<h3 class="hasAnchor">
<a href="#definition-1" class="anchor"></a>Definition</h3>
<p>Reliability is the <strong>consistency of measurement</strong>, i.e. the degree to which a hypothetical repetition of the same measure would yield the same results. As such, reliability is the closeness of a measure to an error free measure. It is not to be confused with validity as a perfectly reliable measure may be completely invalid.</p>
<p>Practically, reliability must be empirically assessed based on a theoretical framework. The dominant - and virtually only - theoretical framework against which to compare empirical reliability results to is the well-known true score framework which provides the underlying justification for the measurement model postulated at the beginning of the <a href="#methods">Methods &amp; Formulae</a> section. Based on the true score framework reliability is defined as</p>
<ol style="list-style-type: decimal">
<li>The amount of population proxy variance, <span class="math inline">\(Var(\bar\eta)\)</span>, relative to the proxy variance, <span class="math inline">\(Var(\hat\eta)\)</span>.</li>
<li>This is identical to the squared correlation between the true score and the proxy: <span class="math inline">\(\rho_{\eta, \hat\eta}^2 = Cor(\eta, \hat\eta)^2\)</span>.</li>
</ol>
<p>This “kind” of reliability is commonly referred to as <strong>internal consistency reliability</strong>.</p>
<p>Based on the true score theory three major types of measurement models are distinguished. Each type implies different assumptions which give rise to the formulae written below. The well-established names for the different types of measurement model provide natural naming candidates for their corresponding reliabilities measure:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Parallel</strong> – Assumption: <span class="math inline">\(\eta_k = \eta \longrightarrow \lambda_k = \lambda\)</span> and <span class="math inline">\(Var(\varepsilon_k) = Var(\varepsilon)\)</span>.</li>
<li>
<strong>Tau-equivalent</strong> – Assumption: <span class="math inline">\(\eta_k = \eta \longrightarrow \lambda_k = \lambda\)</span> and <span class="math inline">\(Var(\varepsilon_k) \neq Var(\varepsilon_l)\)</span>.</li>
<li>
<strong>Congeneric</strong> – Assumption: <span class="math inline">\(\eta_k = \lambda_k\eta\)</span> and <span class="math inline">\(Var(\varepsilon_k) \neq Var(\varepsilon_l)\)</span>.</li>
</ol>
</div>
<div id="formula-1" class="section level3">
<h3 class="hasAnchor">
<a href="#formula-1" class="anchor"></a>Formula</h3>
<p>The most general formula for reliability is the congeneric reliability:</p>
<p><strong>Congeneric reliability</strong> <span class="math display">\[ \rho_C = \frac{Var(\bar\eta)}{Var(\hat\eta_k)} = \frac{(\mathbf{w}'\mathbf{\lambda})^2}{\mathbf{w}'\mathbf{\Sigma}\mathbf{w}}\]</span> Using the assumptions imposed by the tau-equivalent measurement model we obtain the tau-equivalent reliability.</p>
<p><strong>Tau-equivalent reliability</strong> <span class="math display">\[ \rho_T = \frac{\lambda^2(\sum w_k)^2}{\lambda^2(\sum w_k)^2 + \sum w_k^2Var(\varepsilon_k)}
 = \frac{\bar\sigma_x(\sum w_k)^2}{\bar\sigma_x[(\sum w_k)^2 - \sum w_k^2] + \sum w_k^2Var(x_k)}\]</span> where we used the fact that if <span class="math inline">\(\lambda_k = \lambda\)</span> (tau-equivalence), <span class="math inline">\(\lambda^2_k\)</span> equals the average covariance between indicators: <span class="math display">\[\bar\sigma_x = \frac{1}{K(K-1)}\sum^K_{k=1}\sum^K_{l=1} \sigma_{kl}\]</span></p>
<p>Using the assumptions imposed by the parallel measurement model we obtain the parallel reliability:</p>
<p><strong>Parallel reliability</strong> <span class="math display">\[ \rho_P = \frac{\lambda^2(\sum w_k)^2}{\lambda^2(\sum w_k)^2 + Var(\varepsilon)\sum w_k^2} = 
 \frac{\bar\sigma_x(\sum w_k)^2}{\bar\sigma_x[(\sum w_k)^2 - \sum w_k^2] + Var(x)\sum w_k^2} \]</span></p>
<p>In <code>cSEM</code> indicators are always standardized and weights are choosen such that <span class="math inline">\(Var(\hat\eta_k) = 1\)</span>. This significantly simplifies the formulae and <span class="math inline">\(\rho_T = \rho_P\)</span> are in fact identical:</p>
<p><span class="math display">\[\begin{align}
\rho_C &amp;=  (\sum w_k\lambda_k)^2 = (\mathbf{w}'\mathbf{\lambda})^2\\
\rho_T = \rho_P &amp;=  \bar\rho_x(\sum w_k)^2
\end{align}\]</span></p>
<p>where <span class="math inline">\(\bar\rho_x\)</span> is the average correlation between indicators. Consequently, parallel and tau-equivalent reliability are always identical in <code>cSEM</code>.</p>
<p>It is important to realize that <span class="math inline">\(\rho_T\)</span> (<span class="math inline">\(\rho_P\)</span>) based on the average covariance/ correlation (if <span class="math inline">\(x_k\)</span> is standardized as it is always the case in <code>cSEM</code>) are nested within the general formula, however, <strong>if and only if the assumptions of the tau-equivalent and/or the parallel model hold do they represent reliability measures!!</strong> In pratice, these assumptions are unlikely to hold. We therefore explicitly discourage their use as congeneric reliability is virtually always preferable in empirical work! In fact, as shown above, congeneric reliability is identical to the tau-equivalent/parallel reliability <em>if their respective assumptions actually hold</em>, essentially offering robustness against violation of the tau-equivalence and/or parallel measurement assumption.</p>
<div id="confidence-interval" class="section level4">
<h4 class="hasAnchor">
<a href="#confidence-interval" class="anchor"></a>Confidence interval</h4>
<p>Trinchera et al. (2018) proposed a closed-form confidence interval (CI) for the tau-equivalent reliability (coefficient alpha). To compute the CI, set <code>.calculate_ci = TRUE</code> when calling <code><a href="../reference/assess.html">assess()</a></code> or invoke <code><a href="../reference/reliability.html">calculateRhoT(..., .calculate_ci = TRUE)</a></code> directly. The level of the CI can be changed by supplying a single value or a vector of values to <span class="math inline">\(.alpha\)</span>.</p>
</div>
<div id="a-note-on-the-terminology" class="section level4">
<h4 class="hasAnchor">
<a href="#a-note-on-the-terminology" class="anchor"></a>A note on the terminology</h4>
<p>A vast bulk of literature dating back to seminal work by Spearman (e.g., Spearman (1904)) has been written on the subject of reliability. Inevitably, definitions, formulae, notation and terminology conventions are unsystematic and confusing. This is particularly true for newcomers to structural equation modeling or applied users whose primary concern is to apply the appropriate method to the appropriate case without poring over books and research papers to understand each intricate detail.</p>
<p>In <code>cSEM</code> we seek to make working with reliabilities as consistent and easy as possible by relying on a paper by Cho (2016) who proposed, uniform formula-generating methods and a systematic naming conventions for all common reliability measures. Naturally, some of the conventional terminonolgy is deeply entrenched within the nomenclatura of a particular filed (e.g., coefficient alpha alias Cronbach alpha in pychonometrics) such that a new, albeit consistent, naming scheme seems superfluous at best. However, we belief the merit of a “standardized” naming pattern will eventually be helpful to all users as it helps clarify potential missconceptions thus preventing potential missue, such as the (ab)use of Cronbach alpha as a reliability measure for congernic measurement models.</p>
<p>Apart from these considerations, this package takes a pragmatic stance in a sense that we use consistent naming because it naturally provides a consistent naming scheme for the functions and the systematic formula generating methods because they make code maintenance easier. Eventually, what matters is the formula and more so its correct application. To facilitate the translation between different naming systems and conventions we provide a “translation table” below:</p>
<center>
<table class="table">
<caption>Systematic names and common synonymous names for the reliability found in the literature</caption>
<colgroup>
<col width="37%">
<col width="16%">
<col width="46%">
</colgroup>
<thead><tr class="header">
<th align="center">Systematic names</th>
<th>Mathematical</th>
<th align="left">Synonymous terms</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="center">Parallel reliability</td>
<td><span class="math inline">\(\rho_P\)</span></td>
<td align="left">Spearman-Brown formula, Spearman-Brown prophecy, Standardized alpha, Split-half reliability</td>
</tr>
<tr class="even">
<td align="center">Tau-equivalent reliability</td>
<td><span class="math inline">\(\rho_T\)</span></td>
<td align="left">Cronbach’s alpha, <span class="math inline">\(\alpha\)</span>, Coefficient alpha Guttmans <span class="math inline">\(\lambda_3\)</span>, KR-20</td>
</tr>
<tr class="odd">
<td align="center">Congeneric reliability</td>
<td><span class="math inline">\(\rho_C\)</span></td>
<td align="left">Composite reliability, Jöreskog’s <span class="math inline">\(\rho\)</span>, Construct reliability, <span class="math inline">\(\omega\)</span>, reliability coefficient</td>
</tr>
</tbody>
</table>
</center>
</div>
</div>
<div id="implementation-1" class="section level3">
<h3 class="hasAnchor">
<a href="#implementation-1" class="anchor"></a>Implementation</h3>
<p>Function are implemented as: <code><a href="../reference/reliability.html">calculateRhoT()</a></code> and <code><a href="../reference/reliability.html">calculateRhoC()</a></code>. See e.g., <code>?calculateRhoC()</code> for a list of arguments and their description.</p>
</div>
</div>
</div>
<div id="literature" class="section level1">
<h1 class="hasAnchor">
<a href="#literature" class="anchor"></a>Literature</h1>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li>
<a href="#introduction">Introduction</a><ul class="nav nav-pills nav-stacked">
<li><a href="#syntax-options">Syntax &amp; Options</a></li>
      <li><a href="#details">Details</a></li>
      </ul>
</li>
      <li>
<a href="#usage-examples">Usage &amp; Examples</a><ul class="nav nav-pills nav-stacked">
<li><a href="#example1">Example 1</a></li>
      <li><a href="#example2">Example 2</a></li>
      <li><a href="#example3">Example 3</a></li>
      </ul>
</li>
      <li>
<a href="#methods">Methods &amp; Formulae</a><ul class="nav nav-pills nav-stacked">
<li><a href="#average-variance-extracted-ave">Average variance extracted (AVE)</a></li>
      <li><a href="#reliability">Reliability</a></li>
      </ul>
</li>
      <li><a href="#literature">Literature</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Manuel Rademaker, Florian Schuberth.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.3.0.9000.</p>
</div>
      </footer>
</div>

  

  </body>
</html>
