---
title: "Postestimation: Assessing a model"
subtitle: "Using the `assess()` function"
date: "Last edited: `r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    toc: true
    includes:
      before_body: preamble.mathjax.tex
vignette: >
  %\VignetteIndexEntry{using-assess}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
bibliography: ../inst/REFERENCES.bib
---
WARNING: this document is work in progress and may still contain mistakes!
<!-- used to print boldface greek symbols (\mathbf only works for latin symbols) -->
```{r child="preamble-tex.tex", echo=FALSE, include=FALSE}
```
# Introduction
As indicated by the name, `assess()` is used to assess a model estimated using the
`csem()` function.

In **cSEM** model assessement is considered to be any task that in some way or
another seeks to assess the quality of the estimated model *without conducting* 
*a statistical test* (tests are covered by the `test_*` family of functions.
See: [Postestimation: Testing a model][Testing]). 
Quality in this case is taken to be a catch-all term for
all common aspects of model assessment. This mainly comprises
fit indices, reliability estimates, common validity assessment criteria, 
effect sizes, and other 
related quality measures/indices that do not rely on a formal test procedure. Hereinafter, 
we will refer to a generic (fit) index, quality or assessment measure as 
a **quality criterion**.

Currently the following quality criteria are implemented:

- Convergent and discriminant validity assessment:
    - The **average variance extracted** (AVE)
    - The **Fornell-Larcker** criterion
    - The **heterotrait-monotrait ratio of correlations** (HTMT)
- **Congeneric reliability** ($\rho_C$), also known as e.g.: composite reliability,
  construct reliability, (unidimensional) omega, JÃ¶reskog's $\rho$, $\rho_A$,
  or $\rho_B$. 
- **Tau-equivalent reliability** ($\rho_T$), also known as e.g.: Cronbach alpha, alpha, $\alpha$,
  coefficient alpha, Guttman's $\lambda_3$, KR-20. 
- Distance measures
    - The **standardized root mean square residual** (SRMR)
    - The **geodesic distance** (DG)
    - The **squared Euclidian distance** (DL)
    - The **maximum-likelihood distance** (DML)
- Fit indices
    - The **goodness-of-fit index** (GFI) 
    - The **standardized root mean square residual** (SRMR)
    - The **root mean square error of approximation** (RMSEA)
    - The **normed fit index** (NFI)
    - The **non-normed fit index** (NNFI)
    - The **comparative fit index** (CFI)
    - The **incremental fit index** (IFI)
    - The **root mean square outer residual covariance** ($\text{RMS}_{\theta}$) in two alternative versions
- The **Goodness-of-Fit** (GoF) proposed by [@Tenenhaus2004]
- The **variance inflation factors** (VIF) for the structural equations as
  well as for Mode B regressions equations.
- The coefficient of determination and the adjusted coefficient of determination
  ($R^2$ and $R^2_{adj}$)
- A measure of the **effect size**
- **Redundancy analysis** (RA)


For implementation details see the [Methods & Formulae](#methods) section.

## Syntax & Options

```{r eval=FALSE}
assess(
  .object              = NULL,
  .only_common_factors = TRUE,
  .quality_criterion   = c("all", "ave", "rho_C", "rho_C_weighted", "cronbach_alpha", 
                           "cronbach_alpha_weighted", "dg", "dl", "dml",
                           "esize", "gof", "htmt", "r2", "r2_adj", "ra",
                           "srmr", "rho_T", "rho_T_weighted", "vif", 
                           "vifmodeb",  "fl_criterion", "gfi", "rmsea",                                "cfi", "ifi", "nfi", "nnfi", "rms_theta"),
  .model_implied       = TRUE,
  ...
)
```

`.object`

:  An object of class `cSEMResults` resulting from a call to `csem()`.
   
`.quality_criterion`

:  A character string or a vector of character strings naming the quality criterion
   to compute. By default all quality criteria are computed (`"all"`).
   See `assess()` for a list of possible candidates.
  
`.only_common_factors`

:  Logical. Should only concepts modeled as common 
   factors be included when calculating one of the following quality criteria: 
   AVE, the Fornell-Larcker criterion, HTMT, and all reliability estimates. 
   Defaults to `TRUE`.

`.model_implied`

:  Logical. Refers to the calculation of the quality criterion
   $\text{RMS}_{\theta}$. Should the covariance matrix of the constructs be
   calculated taking into account the restrictions of the structural model? 
   Defaults to `TRUE`.
 
`...`

:  Further arguments passed to functions called by `assess()`.
   See [args_assess_dotdotdot][dotdotdot] 
   for a complete list of available arguments.

## Details {#details}
In line with all of **cSEM**'s postestimation functions, `assess()` is a generic 
function with methods for objects of class `cSEMResults_default`, 
`cSEMResults_multi`, `cSEMResults_2ndorder`. In **cSEM**
every `cSEMResults_*` object must also have class `cSEMResults` for internal reasons. 
When using one of the major postestimation functions, method dispatch is therefore technically
done on one of the `cSEMResults_*` class attributes, ignoring the 
`cSEMResults` class attribute. As long as `assess()` is
used directly method dispatch is not of any practical concern to the end-users. 
The difference, however, becomes important if a user seeks to directly invoke an internal function 
which is called by `assess() ` (e.g., `cSEM:::calculateAVE()` or `cSEM:::calculateHTMT()`). 
In this case only objects of class `cSEMResults_default` are accepted as this
ensures a specific structure. Therefore, it is important to remember that
*internal functions are generally **not** generic*!

Some assessment measures are inherently tied to the common factor model. It
is therefore unclear how to interpret their results in the context of a 
composite model. Consequently, their computation is suppressed by default for 
constructs modeled as common factors. Currently, this applies to the following
quality criteria: 

- AVE and validity assessment based theron (i.e., the Fornell-Larcker criterion)
- HTMT and validity assessment based theron
- All reliability measures (congeneric, tau-equivalent, Cronbach's alpha)
 
It is possible to force computation of all quality criteria for constructs
modeled as composites, however, we explicitly warn to interpret results with
caution, as they may not even have a conceptual meaning.

All quality criteria assume that the estimated loadings, construct correlations 
and path coefficients involved in the computation of a specific qualitiy measure
are consistent estimates for their theoretical population counterpart. If
the user deliberately chooses an approach that yields inconsistent estimates (by
setting `.disattenuate = FALSE` in `csem()` when the estimated model contains constructs
modeled as common factors) `assess()` will still estimate
all quantities, however, quantities such as
the AVE or the congeneric reliability $\rho_C$ inherit inconsistency making their 
interpretation at the very least dubious.


# Usage & Examples

Like all postestimation functions `assess()` can be called on any object of
class `cSEMResults`. The output is a named list of the quality criteria given
to `.quality_criterion`. By default all possible quality criteria 
a calculated (`.quality_criterion = "all"`).

## Example 1 {#example1}
## Example 2 {#example2}
## Example 3 {#example3}

# Methods & Formulae {#methods}

This section provides technical details and relevant formulae. For the relevant 
notation and terminology used in this section, see the
[Notation][Notation] and the
[Termionology][Terminology] help files.

## Average variance extracted (AVE) {#ave}
### Definition
The average variance extracted (AVE) was first proposed by @Fornell1981.
Several definitions exist. For ease of comparison to extant literature the most 
common definitions are given below:

- The AVE for a generic construct/latent variable $\eta$ is an estimate of
  how much of the variation of its indicators is due to the 
  assumed latent variable. Consequently, the share of unexplained, 
  i.e. error variation is 1 - AVE.
- The AVE for a generic construct/latent variable $\eta$ is the share of the total 
  indicator variance (i.e., the sum of the indicator variances of all indicators 
  connected to the construct), that is captured by the (indicator) true scores. 
- The AVE for a generic construct/latent variable $\eta$ is the ratio of the 
  sum of the (indicator) true score variances (explained variation) 
  relative to the sum of the total indicator variances (total variation, i.e., 
  the sum of the indicator variances of all indicators connected to the construct).
- Since for the regression of $x_k$ on $\eta_k$, the the R squared ($R^2_k)$ is equal to the
  share of the explained variation of $x_k$ relative to the share of total variation of $x_k$,
  The AVE for a generic construct/latent variable $\eta$ is equal to the average over all $R^2_k$. 
- The AVE for a generic construct/latent variable $\eta$ is the sum of the 
  squared correlation between indicator $x_k$ and
  the (indicator) true score $\eta_k$ relative to the sum of the indicator variances of 
  all indicators connected to the construct in question.

It is important to stress that, although different in wording, all definitions 
are synonymous! 

### Formulae
Using the results and notation derived and defined in the [Notation][Notation] help file,
the AVE for a generic construct is:
$$ AVE = \frac{\text{Sum indicator true score variances}}{\text{Sum indicator variances}} =  \frac{\sum Var(\eta_k)}{\sum Var(x_k)} = \frac{\sum\lambda^2_k}{\sum(\lambda^2_k + Var(\varepsilon_k))}$$
If $x_k$ is standardized (i.e., $Var(x_k) = 1$) the denominator reduces to $K$ 
and the AVE for a generic construct is:
$$ AVE = \frac{1}{K}\sum \lambda^2_k = \frac{1}{K}\sum \rho_{x_k, \eta}^2$$
As an important consequence, the AVE is closely tied to the communality.
**Communality** ($COM_k$) is definied as the proportion of variation in an indicator 
that is explained by its common factor. Empirically, it is the square of 
the standardized loading of the $k$'th indicator ($\lambda^2_k$). Since indicators, 
scores/proxies and subsequently loadings are always standardized in **cSEM**, 
the squared loading is simply the squared correlation between the indicator and 
its related construct/common factor. 
The AVE is also directly related to the **indicator reliability**, 
defined as the squared correlation between an indicator $k$ and its related 
proxy true score (see section [Reliability](#reliability) below),
which is again simply $\lambda^2_k$. Therefore in `cSEM` we always have:

$$ AVE = \frac{1}{K}\sum COM_k = \frac{1}{K}\sum \text{Indicator reliability}_k = \frac{1}{K}\sum\lambda^2_k =  \frac{1}{K}\sum R^2_k $$

### Implementation
The function is implemented as: `calculateAVE()`. It may be called directly 
using R's `:::` mechanism.

### See also

The AVE is the basis for the Fornell-Larcker criterion.

## Fit Indices {#fit_indices}
### Definition
First fit indices for CFA were introduced by @Bentler1980 in 1980. Since then a large number of indices has been defined. Contrary to exact tests of model fit, the purpose of fit indices is to measure the approximate fit of a structural equation model. The values of fit indices are continuous and - in the case of normed indices - between 0 and 1. Fit indices are divided into two classes: 
- 'badness of fit' (resp. 'lack of fit') indices where smaller values indicate a better fit
- 'goodness of fit' indices where higher values represent a better fit.

Whereas fit indices in CFA were subject of several studies analyzing their empirical or theoretical properties, barely anything is known about fit indices in composite-based methods. Much research remains to be done (which also applies to the area of fit indices in CFA). A highly controversial issue are cutoff criteria for fit indices. For CFA, based on an empirical study, cutoff criteria for some fit indices were proposed by @Hu1999. Cutoff criteria for composite-based methods are still to be analyzed. Using `assess()` to calculate fit indices, the user should always keep in mind that the value of a fit index is just an indication of good or bad fit. Other aspects related to model fit have to be considered. It is inappropriate to make a binary decision about acceptance or rejection of a model by comparing the value of the fit index with a (more or less) arbitrary cutoff value.  

The definitions of those fit indices which can be calculated by `assess()` are given below:
- The **goodness-of-fit index** (GFI) measures the increase in fit when specifying the model under consideration compared with no model at all. 
- The **standardized root mean square residual** (SRMR) is the square root of the mean of squared residual correlations.
- The **root mean square error of approximation** (RMSEA) is the square root of the discrepancy due to approximation per degree of freedom.
- The **normed fit index** (NFI) measures the increase in fit when specifying the model under consideration relative to the fit of a certain baseline model (also called null model).
- The **non-normed fit index** (NNFI) accounts for the degrees of freedom of the involved models. It is the ratio of the distance between the fit of the baseline model and the fit of the specified model (each per degree of freedom) and the distance beetween the fit of the baseline model and the expected fit of the specified model (each per degree of freedom).
- The **comparative fit index** (CFI) estimates the relative decrease in non-centrality when specifying the model under consideration instead of the baseline model. 
- The **incremental fit index** (IFI) is the ratio of the distance between the fit of the baseline model and the fit of the specified model and the distance beetween the fit of the baseline model and the expected fit of the specified model. Its definition differs only slighty from the definition of NNFI.
- The **root mean square outer residual covariance** ($\text{RMS}_{\theta}$) is defined as the square root of the mean squared covariances of the residuals of the outer model. The calculation of the indicator's residual covariance matrix involves the calculation of the construct's covariance matrix. In @Lohmoeller1989, this is not stated more precisely. That is why, two alternatives are possible:
    - the restrictions of the structural model are taken into account basing       the calculation on the model-implied construct covariance matrix
    - the restrictions of the structural are not taken into account basing        the calculation on the empirical construct covariance matrix

It should be noted that not all of the above fit indices were originally designed for the use with composite-based methods of structural equation modeling (CCA). The indices RMSEA and CFI are non-centrality based and require specific assumptions on model and data typically made in CFA. The same applies for IFI and NNFI since their calculation relies on the properties (primarily the expectation) of the test statistic when data follow a normal distribution. In general, those assumptions and requirements are not valid for CCA. For this reason, the intuition behind these indices does not hold for composite-based SEM. Nevertheless, calculation of these indices is also possible in this case. Whether the values of these indices are then still meaningful and can be used for assessment of model fit is an open question. Furthermore, values of fit indices in CCA and CFA may not be compared. Users should always keep this aspect and the general limitations of fit indices in mind. It is worthwile to emphasize another time that the usage of fit indices should happen in a careful and reflected way.
 
### Formulae

### Implementation
The functions are implemented as: `calculateCFI()`, `calculateNFI()`, `calculateNNFI()`, `calculateIFI()`, `calculateGFI()`, `calculateRMSEA()`, `calculateRMSTheta()`, `calculateSRMR()`. It may be called directly 
using R's `:::` mechanism.

### See also

Several fit indices require a fitting function, that is to say a distance measure like the geodesic distance, the squared Euclidian distance or the maximum-likelihood distance.

## Reliability {#reliability}

### Definition
Reliability is the **consistency of measurement**, i.e. the degree to which a hypothetical
repetition of the same measure would yield the same results. As such, reliability
is the closeness of a measure to an error free measure. It is not to be confused 
with validity as a perfectly reliable measure may be invalid.

Practically, reliability must be empirically assessed based on a theoretical
framework. The dominant - and virtually only - theoretical framework against which 
to compare empirical reliability results to is the well-known true score framework 
which provides the foundation for the measurement model described 
in the [Notation][Notation] help file. 
Based on the true score framework and using the terminology and notation of
the [Notation][Notation] and [Termniology][Terminology] help files, reliability 
is defined as:

1. The amount of proxy true score variance, $Var(\bar\eta)$, relative to the 
   the proxy or test score variance, $Var(\hat\eta)$.
2. This is identical to the squared correlation between the common 
   factor and its proxy/composite or test score: $\rho_{\eta, \hat\eta}^2 = Cor(\eta, \hat\eta)^2$.

This "kind" of reliability is commonly referred to as 
**internal consistency reliability**.

Based on the true score theory three major types of measurement models are 
distinguished. Each type implies different assumptions which give rise
to the formulae written below. The well-established names for the different 
types of measurement model provide natural naming candidates for their corresponding
(internal consistency) reliabilities measure:

1. **Parallel** -- Assumption: $\eta_k = \eta \longrightarrow \lambda_k = \lambda$ and $Var(\varepsilon_k) = Var(\varepsilon)$.
1. **Tau-equivalent** -- Assumption: $\eta_k = \eta \longrightarrow \lambda_k = \lambda$ and $Var(\varepsilon_k) \neq Var(\varepsilon_l)$.
1. **Congeneric** -- Assumption: $\eta_k = \lambda_k\eta$ and $Var(\varepsilon_k) \neq Var(\varepsilon_l)$.

In principal the test score $\hat\eta$ is weighted linear combinations of 
the indicators (i.e. a proxy or stand-in for the true score/common factor). 
Historically, however, the test score is generally assumed to be a simple sum 
score (i.e a weighted sum of indicators with all weights assumed to be equal to 
one). Hence, well-known reliability measures such as Cronbach's alpha are definied
with respect to a test score that indeed represents a simple sum score. 
Yet, all reliability measures originally developped assuming a sum score may 
equally well be computed with respect to a composite 
(i.e., a weighted score, with weights not necessarily equal to one).

Apart form the distinction between congeneric and tau-equivalent reliability 
(Cronbach's alpha) we therefore distinguish between reliability estimates based on
a test score (composite) that uses the weights of the weight approach used 
to obtain `.object` and a test score (proxy) based on unit weights. The former
is indicated by adding "**weighted**" to the original name.

### Formulae

The most general formula for reliability is the **weighted congeneric reliability**:

$$ \rho_{C; \text{weighted}} = \frac{Var(\bar\eta)}{Var(\hat\eta_k)} = \frac{(\bm{w}'\bm{\lambda})^2}{\bm{w}'\bm{\Sigma}\bm{w}}$$
Assuming $w_k = 1$, i.e. unit weights, the "classical" formula for congeneric reliability follows:
$$ \rho_C = \frac{Var(\bar\eta)}{Var(\hat\eta_k)} = \frac{\left(\sum\lambda_k\right)^2}{\left(\sum\lambda_k\right)^2 + Var(\bar\varepsilon_k)}$$
Using the assumptions imposed by the tau-equivalent measurement model we obtain
the **weighted tau-equivalent reliability (weighted Cronbach's alpha)**:

$$ \rho_{T; \text{weighted}}  = \frac{\lambda^2(\sum w_k)^2}{\lambda^2(\sum w_k)^2 + \sum w_k^2Var(\varepsilon_k)}
 = \frac{\bar\sigma_x(\sum w_k)^2}{\bar\sigma_x[(\sum w_k)^2 - \sum w_k^2] + \sum w_k^2Var(x_k)}$$
where we used the fact that if $\lambda_k = \lambda$ (tau-equivalence), 
$\lambda^2_k$ equals the average covariance between indicators:
 $$\bar\sigma_x = \frac{1}{K(K-1)}\sum^K_{k=1}\sum^K_{l=1} \sigma_{kl}$$
Again, assuming $w_k = 1$, i.e. unit weights, the "classical" formula for 
tau-equivalent reliability (Cronbach's alpha) follows:
$$ \rho_T = \frac{\lambda^2K^2}{\lambda^2K^2 + \sum Var(\bar\varepsilon_k)}
 = \frac{\bar\sigma_xK^2}{\bar\sigma_x[K^2 - K] + K Var(x_k)}$$
Using the assumptions imposed by the parallel measurement model we obtain
the **parallel reliability**:

$$ \rho_P = \frac{\lambda^2(\sum w_k)^2}{\lambda^2(\sum w_k)^2 + Var(\varepsilon)\sum w_k^2} = 
 \frac{\bar\sigma_x(\sum w_k)^2}{\bar\sigma_x[(\sum w_k)^2 - \sum w_k^2] + Var(x)\sum w_k^2} $$

In **cSEM** indicators are always standardized and weights are choosen such 
that $Var(\hat\eta_k) = 1$. This significantly simplifies the formulae and 
$\rho_T = \rho_P$ are in fact identical:

$$
\begin{align}
\rho_{C; \text{weighted}} &= (\sum w_k\lambda_k)^2 = (\bm{w}'\bm{\lambda})^2 \\
\rho_C &=  (\sum \lambda_k)^2 \\
\rho_{T; \text{weighted}} = \rho_{P; \text{weighted}} &=  \bar\rho_x(\sum w_k)^2 \\
\rho_{T} = \rho_P = \bar\rho_x K^2
\end{align}
$$

where $\bar\rho_x = \bar\sigma_x$ is the average correlation between indicators. Consequently,
parallel and tau-equivalent reliability are always identical in **cSEM**.

<!-- It is important to realize that $\rho_T$ ($\rho_P$) based on the average covariance/ -->
<!-- correlation (if $x_k$ is standardized as it is always the case in **cSEM**) are nested within the  -->
<!-- general formula, however, **if and only if the assumptions of the tau-equivalent -->
<!-- and/or the parallel model hold do they represent reliability measures.** -->
<!-- In pratice, these assumptions are unlikely to hold. We therefore explicitly discourage -->
<!-- their use as congeneric reliability is virtually always preferable in empirical -->
<!-- work! In fact, as shown above, congeneric reliability is identical to the  -->
<!-- tau-equivalent/parallel reliability *if their respective assumptions actually hold*, -->
<!-- essentially offering robustness against violation of the tau-equivalence and/or -->
<!-- parallel measurement assumption. -->

#### Closed-form confidence interval

@Trinchera2018 proposed a closed-form confidence interval (CI) for
the tau-equivalent reliability (Cronbach's alpha). To compute the CI, set
`.closed_form_ci = TRUE` when calling `assess()` or invoke 
`calculateRhoT(..., .closed_form_ci = TRUE)` directly. The level of the CI
can be changed by supplying a single value or a vector of values to `.alpha`.

#### A note on the terminology

A vast bulk of literature dating back to seminal work by Spearman (e.g., Spearman (1904))
has been written on the subject of reliability. Inevitably, definitions, formulae, 
notation and terminology conventions are unsystematic and confusing. This is 
particularly true for newcomers to structural equation modeling 
or applied users whose primary concern is to apply the appropriate method 
to the appropriate case without poring over books and research papers
to understand each intricate detail.

In **cSEM** we seek to make working with reliabilities as consistent and easy as
possible by relying on a paper by @Cho2016 who proposed uniform 
formula-generating methods and a systematic naming conventions for all
common reliability measures. Naturally, some of the conventional terminonolgy 
is deeply entrenched within the nomenclatura of a particular filed (e.g., coefficient
alpha alias Cronbach's alpha in pychonometrics) such that a new, albeit consistent,
naming scheme seems superfluous at best. However, we belief the merit of a 
"standardized" naming pattern will eventually be helpful to all users as it
helps clarify potential missconceptions thus preventing potential missue, such as
the (ab)use of Cronbach alpha as a reliability measure for congernic measurement models.

Apart from these considerations, this package takes a pragmatic stance
in a sense that we use consistent naming because it naturally provides
a consistent naming scheme for the functions and the systematic formula generating
methods because they make code maintenance easier. Eventually, what matters is 
the formula and more so its correct application. To facilitate the translation between
different naming systems and conventions we provide a "translation table" below:

<center>
---------------------------------------------------------------------------------------------------------------
 Systematic names                  Mathematical                   Synonymous terms
 
--------------------------------- ------------------------------ ----------------------------------------------
 Parallel reliability              $\rho_P$                      Spearman-Brown formula, Spearman-Brown prophecy,
                                                                 Standardized alpha, Split-half reliability

 Tau-equivalent reliability        $\rho_T$                      Cronbach's alpha, $\alpha$, Coefficient alpha
                                                                 Guttmans $\lambda_3$, KR-20
  
 Congeneric reliability            $\rho_C$                      Composite reliability, JÃ¶reskog's $\rho$,
                                                                 Construct reliability, $\omega$,
                                                                 reliability coefficient, Dillon-Goldsteins's $\rho$
                                                                 
 Weighted Congeneric reliability   $\rho_{C;\text{weighted}}$    $\rho_A$, $\rho_B$
-----------------------------------------------------------------------------------------------------------------

Table: Systematic names and common synonymous names for the
reliability found in the literature
</center>

### Implementation
The functions are implemented as: `calculateRhoC()` and `calculateRhoT()`. 
They may be called directly using R's `:::` mechanism.

## The Goodness of Fit (GoF)
### Definition
Calculate the Goodness of Fit (GoF) proposed by @Tenenhaus2004. 
Note that, contrary to what the name suggests, the GoF is **not** a 
measure of (overall) model fit in a $\chi^2$-fit test sense. 
See e.g. @Henseler2012a for a discussion.

### Formulae
The GoF is defined as:

$$\text{GoF} = \sqrt{\varnothing \text{COM}_k \times \varnothing R^2_{structural}} = 
\sqrt{\frac{1}{k}\sum^K_{k=1} \lambda^2_k + \frac{1}{M} \sum^M_{m = 1} R^2_{m;structural}} $$
where $COM_k$ is the communality of indicator $k$, i.e. the variance in the indicator 
that is explained by its connected latent variable and $R^2_{m; structural$ the 
R squared of the $m$'th equation of the structural model.

### Implementation
The function is implemented as: `calculateGoF()`. It may be called directly 
using R's `:::` mechanism.

## The heterotrait-monotrait-ratio of correlations (HTMT)

### Definition
The heterotrait-monotrait ratio of correlations (HTMT) was first proposed by  
@Henseler2015 to assess convergent and discriminant validity.

### Formulae

### Implementation
The function is implemented as: `calculateHTMT()`. It may be called directly 
using R's `:::` mechanism.

# Literature

[Testing]: https://m-e-rademaker.github.io/cSEM/articles/Using-test.html
[Notation]: https://m-e-rademaker.github.io/cSEM/articles/Notation.html
[Terminology]: https://m-e-rademaker.github.io/cSEM/articles/Terminology.html
[dotdotdot]: https://m-e-rademaker.github.io/cSEM/reference/args_assess_dotdotdot.html

<!-- A third reliability measure is **Parallel reliability** ($\rho_P$), also known -->
<!-- as e.g.: split-half reliability,  -->
<!-- Spearman-Brown formulae/prophecy, standarized alpha. In `cSEM` parallel  -->
<!-- reliability is always identical to tau-equivalent reliability as indicators -->
<!-- are always standardized. Hence, only $\rho_T$ is reported. -->
