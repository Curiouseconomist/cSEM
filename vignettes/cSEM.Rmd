---
title: "Introduction to cSEM"
subtitle: "Concepts, terminonlogy, and notation"
date: "Last edited: `r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    toc: true
    includes:
      before_body: preamble.mathjax.tex
vignette: >
  %\VignetteIndexEntry{csem-basic}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: ../inst/REFERENCES.bib
---

WARNING: this document is work in progress and may still contain mistakes!
<!-- used to print boldface greek symbols (\mathbf only works for latin symbols) -->
```{r child="preamble-tex.tex", echo=FALSE, include=FALSE}
```
# Preface

Structural equation modeling (SEM) has been used for over a century
across a multitude of research fields such as (among others) psychology, 
sociology, or business research. 

As an almost inevidable consequence, a different terminology system and a distinct 
mathematical notation has evolved within each field over the years. This 
"terminology mess" is one of the major obstacles in interdisciplinary
research and (scientific) debates, hampers a broader understanding of 
methodolocial issues, and, even worse,
promotes systematic missue (e.g. the use of Cronbach's alpha as an estimator for
congeneric reliability). This is particulary
true for those new to SEM or casual applied users who are often overwhelmed
by terminology of their own field only to find that the term they
just thought to have finally grasped is definied differently in another field, 
making the confusion complete.

Ultimately, this is a matter of (mis)communication which, in our opinion, can 
only be satisfactorily solved by establishing a clear, unambigious definition 
for each term and symbol that is used within the package. We emphasize that
we do not seek to impose "our" conventions, nor do we claim they are the
"correct" convention, but merly seek to make communication
between us (the authors of the package) and you (the package user) as
unambigious and error-free as possible. 

This introduction therefore provides two glossaries:

1. The [termionology glossary](#terminology) containing each term that we 
  think should be definied and explained in order to make sure
  package users understand complementary help files or vignettes the way
  the package authors intended them to be understood.
1. The [mathematical notation glossary](#notation) containing each mathematical 
  symbol used in the package documentation alongside a definition.
         
The package is designed based on a set of principles that are partly in contrast 
to common open-source or commerical software offering similar content. This 
introduction serves to explain these principles, to contrast them to existing software,
and to point readers to further reading. Lastly, to use **cSEM** effectively, 
it is helpful to understand its design. Hence, the package architecure and design
as well as what we consider the "cSEM workflow" is discussed.

# Composite-based structural equation modeling 
## What is structural equation modeling (SEM)
[Structural equation modeling (SEM)](#terminology) is about modeling the (causal) relationship 
between [concepts](#terminology) (an entity defined by a conceptual definition, 
e.g. "emotional stability", "self-confidence", "customer loyalty") with other 
[concepts](#terminology) and/or observable quantities generally referred to as 
[indicators, manifest variables or items](#indicators). Broadly speaking, two modeling 
approaches exist. We refer to the first as [the latent variable model](#lvmodel) and to 
the second as [the composite model](#cmodel). 
Each approach entails a set of methods, estimators, test,
and evaluation criteria as well as a specific terminology that may or may not 
be adequate within the realm of the other approach. 

### The latent variable or common factor model {#lvmodel}
Assuming there are $J$ [concepts](#terminology) and $K$ indicators, 
the fundamental feature 
of the latent variable model is the assumption of the existence of a set of $J$ 
[latent variables (common factors)](#terminology) that each serve as a representation of
one of the $J$ [concepts](#terminology) to be studied in a sense that each 
[latent variable](#terminology) is at least 
partly (causally) responsible for the manifestations of a set of indicators which
are supposed to measure the [concept](#terminology) in question. 
The entirety of these measurement relations is captured by the **measurement model**
which relates indicators to [latent variables](#terminology)
according to the researchers theory of how observables are related to the 
[concepts](#terminology) in question. The entirety of the relationships between 
[concepts](#terminology) (i.e., its representation in a statistical model, the [construct](#terminology)) is 
captured by the **structural model** whose parameters are usually at the center 
of the researchers interest. Caution is warranted though as the [common factor](#terminology)
and its respective [concept](#terminology) are not the same thing. Within the covariance-based 
literature [concept](#terminology), [construct](#terminology), [latent variable](#terminology) 
and its representation the 
[common factor](#terminology) have often been used interchangeably 
[@Rigdon2012; @Rigdon2016; @Rigdon2019].
This will not be the case here and readers are explicitly made aware of the 
fact that [concepts](#terminology) are the abstract entity which *may* be modeled by 
a [common factor](#terminology), however, no assertion as to the correctness of this approach 
in terms of "closeness of the [common factor](#terminology) and its related [concept](#terminology)" are made.

Parameters in latent variable models are usually retrieved by 
(full-information) maximum likelihood. The idea is to find parameters
such that the difference between the model-implied 
and the empirical indicator covariance matrix is minimized. Such estimation methods
are therefore usually refered to as covariance-based methods.

### The composite model {#cmodel}
The second approach is known as the composite model. As opposed to the latent
variable or common factor model, composite do not presuppose the existence of 
a latent variable. Hence, designed entities (artifacts) such as the 
"OECD Better Life Index" that arguably have no latent counterpart may be 
adequatly described by a composite, i.e the linear combination of observables defining
the composite. Composites may also be formed to represent latent variables/common
factor (or more precisely a concepts modeled as a common factors) in which case
the composite serves as a proxy or stand-in for the the latent variable.
However, in cSEM, the term "composite model" is only used to refer to a model
in a the former sense, i.e. a model in which the composite is a direct representation
of concept/construct!


## What is composite-based SEM?

Composite-based SEM is the entirety of methods, approaches, procedures, algorithms
that in some way or another involve linear compounts (composites), i.e. linear combinations of 
observables when retriving (estimating) quantities of interest such as the 
coefficents of the structural model.  Its is important to clearly distiguish between the composite
model and what we call composite-based SEM. They are not the same.
While the former is "only" *__a__* *model* relating concepts to observables, the latter
simply states that composites (linear compounts, i.e. (weighted linear combinations
of observables) are used to retrive quantities of interest, given
a specific modeling approach! Hence, composite-based SEM thus covers common 
factor as well as composite models!

As sketched above, common factor and composite models fundamentally differ in how the relation
between observables and concepts is modeled. Naturally, results and, most notably,
their (correct/meaningful) interpretation critically hinge on what type of model the user 
specifies. Across the package we therefore strictly distigusih between

- Concepts modeled as common factors (or alternatively latent variables)
- Concepts modeled as composites

These phrases will repeateatly appear in help files and other complementary files. It
is therefore crucial to remember what they supposed to convey.

# Using cSEM
## Principles underlying cSEM 

cSEM is based on a number of principles, that have had a profound influence on
the design of the package.

### Model vs. Estimation

The way different concepts and their relationship 
are *modeled* is strictly distinct from how they are *estimated*. Hence we
strictly distinguish between concepts modeled as common factors (or composites)
and the actual estimation for **a given model**.

In cSEM we make use of the lavaan model syntax but redefine the operator `<~` to mean
that the concept to its left is modeled as a composite (not a formative 
common factor). The operator `=~` is used to mean that the concept to its
left is modeled as a common factor.

**Example**
```{r, eval = FALSE}
model <- "
## Structural model
eta2 ~ eta1

## Measurement model
eta1 <~ item1 + item2 + item3 # eta1 is modeled as a composite 
eta2 =~ item4 + item5 + item6 # eta2 is modeled as a common factor
"
```



### Composites in a composite model vs. composite in a common factor model and disattenuation
By virtue of the package, cSEM uses composite-based estimators only. Depending
on the model choosen, linear compounts may therefore either serve as a
composite as part of the composite model or as a proxy/stand-in for a common factor.
If a concept is modeled as a common factor, proxy correlations, proxy-indicator
correlations and path coeffcients are inconsistent estimates for their supposed
construct level counterparts (construct correlations, loadings and 
path coefficients) unless the proxy is a perfect representation of its 
construct level counterpart. This is commonly refered to as **attentuation bias**.  
Several approaches have been suggested to correct for these biases. 
In cSEM estimates are correctly dissattenuated by default if any of the 
concepts involved are modeled as a common factor! Disattentuation is controlled
by the `.disattenuate` argument of `csem()`

**Example**
```{r eval=FALSE}
model <- "
## Structural model
eta2 ~ eta1

## Measurement model
eta1 <~ item1 + item2 + item3
eta2 =~ item4 + item5 + item6 
"

# Identical
csem(threecommonfactors, model)
csem(threecommonfactors, model, .disattenuate = TRUE)

# To supress automatic disattenuation
csem(threecommonfactors, model, .disattenuate = FALSE)
```

Note that since `.approach_weights = "PLS-PM"` and `.disattentuate = TRUE` 
by default (see for [The role of the weightning scheme and partial least squares (PLS)](#roleofpls) below) 
and one of the concepts in the model above is modeled as a common factor,
composite (proxy) correlations, loadings and path coefficients are adequately 
disattenuated using the correction approach commonly known as **consistent partial
least squares (PLSc)**. 
If `.disattenuate = FALSE` or all concepts are modeled as composites "proper"
PLS values are returned. To learn more about disattenuation, see the Methods 
and Formulae section of the respective weightning approach.

### The role of the weightning scheme and partial least squares (PLS) {#roleofpls}

In principal, any weighted combination of 
appropriately choosen observables can be used to estimate structural relationships
between these compounts. Hence, any conceptual or methodological issue
disscussed based on a composite build by a given (weightning) approach may 
equally well be discussed for any other potential weightning scheme. The 
appropriatness or potential superiority of a specific weightning approach such as
"unit weights" (sum scores) over another such as "partial least squares path modeling" (PLS)
is therfore to some extent a question of *relative* appropriatness and
*relative* superiority. 

As a notable consequence, we belief that well known approach such partial least 
squares path modeling (PLS) and generalized structured component analysis (GSCA) 
are - contrary to common belief - best exclusivly understood as prescriptions for forming
linear compunts based on observables, i.e. as weightning approaches. In cSEM this
is reflected by the fact that `"PLS"` and `"GSCA"` are choices of the `.approach_weights`
argument.

```{r eval=FALSE}
model <- "
## Structural model
eta2 ~ eta1

## Measurement model (pure composite model)
eta1 <~ item1 + item2 + item3
eta2 <~ item4 + item5 + item6 
"

### Currently the following weight approaches are implemented
# Partial least squares path modeling (PLS)
csem(threecommonfactors, model, .approach_weights = "PLS-PM") # default

# Generalized canonical correlation analysis (Kettenring approaches)
csem(threecommonfactors, model, .approach_weights = "SUMCORR")
csem(threecommonfactors, model, .approach_weights = "MAXVAR")
csem(threecommonfactors, model, .approach_weights = "SSQCORR")
csem(threecommonfactors, model, .approach_weights = "MINVAR") 
csem(threecommonfactors, model, .approach_weights = "GENVAR")

# Generalized structured component analysis (GSCA)
csem(threecommonfactors, model, .approach_weights = "GSCA")

# Principal component analysis (PCA)
csem(threecommonfactors, model, .approach_weights = "PCA")

# Factor score regression (FSR) using "unit", "bartlett" or "regression" weights
csem(threecommonfactors, model, .approach_weights = "unit")
csem(threecommonfactors, model, .approach_weights = "bartlett") 
csem(threecommonfactors, model, .approach_weights = "regression")
```

## The cSEM-Workflow

Working with cSEM consists of 4 steps

- Preparing the data
- Specifying the model, 
- Using the `csem()` function
- Applying the postestimation functions

The following sections explain each step in detail.

### Preparing the data

### Specfiying a model
Models are defined using [lavaan syntax](http://lavaan.ugent.be/tutorial/syntax1.html).
**cSEM** handles linear, nonlinear and hierachical models. Syntax for each
model is illustrated below using variables of the build-in `satisfaction` dataset. 
For more information please see the [lavaan syntax tutorial](http://lavaan.ugent.be/tutorial/syntax1.html).

#### Linear model
A typical linear model would looks like this: 
```{r eval=FALSE}
model <- "
# Structural model
EXPE ~ IMAG
QUAL ~ EXPE
VAL  ~ EXPE + QUAL
SAT  ~ IMAG + EXPE + QUAL + VAL 
LOY  ~ IMAG + SAT

# Measurement model

IMAG <~ imag1 + imag2 + imag3                  # composite
EXPE <~ expe1 + expe2 + expe3                  # composite
QUAL <~ qual1 + qual2 + qual3 + qual4 + qual5  # composite
VAL  <~ val1  + val2  + val3                   # composite
SAT  =~ sat1  + sat2  + sat3  + sat4           # common factor
LOY  =~ loy1  + loy2  + loy3  + loy4           # common factor
"
```
Note that the opeartor `<~` tells **cSEM** that the concept to its left is modelled as a composite;
the operator `=~` tells **cSEM** that the concept to its left is modelled
as a common factor.

#### Nonlinear model

Nonlinear terms are specified as interactions using the colon operator `:`. 
Non linear terms include interactions and exponential terms. The latter is 
described in model syntax as an "interaction with itself", e.g., 
`x_1^3 = x1:x1:x1`. Currently the following terms are allowed

- Single, e.g., `eta1`
- Quadratic, e.g., `eta1:eta1`
- Cubic, e.g., `eta1:eta1:eta1`
- Two-way interaction, e.g., `eta1:eta2`
- Three-way interaction, e.g., `eta1:eta2:eta3`
- Quadratic and two-way interaction, e.g., `eta1:eta1:eta3`

A simple example would look like this:
```{r}
model <- "
# Structural model
EXPE ~ IMAG + IMAG:IMAG

# Measurement model
EXPE <~ expe1 + expe2
IMAG <~ imag1 + imag2
"

```

#### Hiearchical (second order) model

Currently only second-order models are supported. Specification of the 
second order construct takes place in the measurement model. 
```{r, eval=FALSE}
model <- "
# Structural model
SAT ~ QUAL
VAL ~ SAT + QUAL

# Measurement model
EXPE <~ expe1 + expe2

SAT =~ sat1 + sat2
VAL =~ val1 + val2
QUAL =~ IMAG + EXPE
IMAG <~ imag1 + imag2
"
```
In this case
`QUAL =~ IMAG + EXPE` is modeled as a second-order common factor.

### Using `csem()`

`csem()` is the central function of the package. Although it is possible to 
estimate a model using individual functions called by `csem()` (such as `parseModel()`, `processData()`, `calculateWeightsPLS()` etc. ), it is virturally
always easier, saver and quicker to use `csem()` instead.

`csem()` accepts all models and data types described above. The result of a call to
`csem()`is always an object of class `cSEMResults`. Technically, the resulting
object has an additional class attribute, namely `cSEMResults_default`, `cSEMResults_multi`
or `cSEMResults_2ndorder` that depends on the type of model and/or data provided, 
however, users usually dont need to worry since postestimation
functions automatically work on all classes. 

The simplest possible call to `csem()` involes a dataset and a model:
```{r warning=FALSE, message=FALSE}
require(cSEM)

model <- "
# Path model / Regressions 
eta2 ~ eta1
eta3 ~ eta1 + eta2

# Measurement model
eta1 =~ y11 + y12 + y13
eta2 =~ y21 + y22 + y23
eta3 =~ y31 + y32 + y33
"
  
a <- csem(.data = threecommonfactors, .model = model)
a
```
This is equal to:
```{r, eval=FALSE}
csem(
   .data                        = threecommonfactors,
   .model                       = model,
   .approach_cor_robust         = "none",
   .approach_nl                 = "sequential",
   .approach_paths              = "OLS",
   .approach_weights            = "PLS-PM",
   .conv_criterion              = "diff_absolute",
   .disattenuate                = TRUE,
   .dominant_indicators         = NULL,
   .estimate_structural         = TRUE,
   .id                          = NULL,
   .iter_max                    = 100,
   .normality                   = TRUE,
   .PLS_approach_cf             = "dist_squared_euclid",
   .PLS_ignore_structural_model = FALSE,
   .PLS_modes                   = NULL,
   .PLS_weight_scheme_inner     = "path",
   .reliabilities               = NULL,
   .starting_values             = NULL,
   .tolerance                   = 1e-05,
   .resample_method             = "none", 
   .resample_method2            = "none",
   .R                           = 499,
   .R2                          = 199,
   .handle_inadmissibles        = "drop",
   .user_funs                   = NULL,
   .eval_plan                   = "sequential",
   .seed                        = NULL,
   .sign_change_option          = "no"
    )
```
See the `csem()` documentation for details.

#### Inference

By default no inferential quantities are calculated since most composite-based approaches
do not have closed-form solutions for standard errors. `cSEM` relies on the 
 `bootstrap` or `jackknife`  to estimate standard errors, test statistics, 
and critical quantiles.

`cSEM` offers two ways to compute resamples:

1. Inference can be done by first setting `.resample_method` to 
`"jackkinfe"` or `"bootstrap"` and subsequently using `summarize()` or `infer()`.
2. The same result is achieved by passing a `cSEMResults` object to `resamplecSEMResults()`
and subsequently using `summarize()` or `infer()`.

```{r eval=FALSE}
# Setting `.resample_method`
b1 <- csem(.data = satisfaction, .model = model, .resample_method = "bootstrap")
b2 <- resamplecSEMResults(a)
```

Several confidence intervals are implemented, see `?infer()`:

```{r eval=FALSE}
summarize(b1)
infer(b1, .quantity = c("CI_standard_z", "CI_percentile")) # no print method yet
```

Both bootstrap and jackknife resampling support platform-independent multiprocessing 
as well as random seeds via the [future framework](https://github.com/HenrikBengtsson/future). 
For multiprocessing simply set `eval_plan = "multiprocess"` in which case the maximum number of available cores
is used if not on Windows. On Windows as many separate R instances are opened in the
backround as there are cores available instead. Note
that this naturally has some overhead so for a small number of resamples multiprocessing
will not always be faster compared to sequential (single core) processing (the default).
Seeds are set via the `.seed` argument.

```{r eval=FALSE}
b <- csem(
  .data            = satisfaction,
  .model           = model, 
  .resample_method = "bootstrap",
  .R               = 999,
  .seed            = 98234,
  .eval_plan       = "multiprocess")
```

### Applying a postestimation function

Currently, there are 4 major postestimation function and 3 test-family functions:

- `assess()`
- `predict()`
- `summarize()`
- `verify()`

- `testOMF()`
- `testMGD()`
- `testMICOM()`

Postestimation functions are generic function with methods for objects of class 
`cSEMResults_default`, `cSEMResults_multi`, `cSEMResults_2ndorder`. In `cSEM`
every `cSEMResults_*` object must also have class `cSEMResults` for internal reasons. 
When using one of the major postestimation functions, method dispatch is therefore 
technically done on one of the `cSEMResults_*` class attributes, ignoring the 
`cSEMResults` class attribute. As long as a postestimation function is
used directly method dispatch is not of any practical concern to the end-user. 
The difference, however, becomes important if a user seeks to directly invoke an internal function 
which is called by one of the postestimation functions (e.g., `calculateAVE()` or 
`calculateHTMT()` as called by `assess()`). 
In this case only objects of class `cSEMResults_default` are accepted as this
ensures a specific structure. Therefore, it is important to remember that
*internal functions are generally **not** generic.*

# Terminology & Notation 
The termionology and (mathematical) notiation used in complementatry help files
is described here. 

## Terminology {#terminology}

Common factor

:   A common factor or latent variable is a type of construct. The defining feature of a
    common factor is the idea that the common factor is the common underlying 
    (latent) cause of the realizations of a set of indicators that are said to measure 
    the common factor. The common factor model is also referred to as 
    reflective measurement model. Accordingly, indicators related to the common factor
    are modeled as measurement error-prone manifestations of a common variable, 
    cause or factor. The idea of a common factor is closely related to the true
    score theory. Although, there a subtle conceptional differences, the terms common factor,
    true score, and latent variable are used synommenoulsy in **cSEM**.

Composite

:   A composite is a weighted sum of its related indicators. Composites may 
    either serve as constructs in their own right or as proxies for a latent variable. 
    The nature of the composite is therefore defined by the type of (measurement) 
    model. If composites are error-free representations 
    of a concept we refer to the measurement model as composite measurement. If 
    composites are used as stand-ins, the measurement model is called 
    causal-formative (Henseler, 2017). 
    
Composite-based methods 

:   Composite-based methods or composite-based SEM refers to the entirety of methods 
    centered around the use as of composites (linear compounts of observables) 
    as stand-ins or error-free representations for the concepts under investigation.
    Factor-based methods are also called
    variance-based methods as focal parameters are usually retrived such that explained 
    variance is maximized.
    
Concept

:   An entity defined by a conceptual/theoretical definition. In line with Rigdon (2016)
    variables representing or subsuming a concept are called **conceptual variables**. 
    The precise nature/meaning of a conceptual variable depends on 
    "differnt assumptions, philosophies or worldviews [of the researcher]" 
    (Rigdon, 2016, p. 2). Unless otherwise stated, in **cSEM**, it is sufficient
    to think of concepts as entities that exist simply because they 
    have been defined. Hence, abstract terms such as "loyalty" or "depression" as 
    well as designed entities (artifacts) such as the "OECD Better Life Index"
    are covered by the definition.

Construct

:   Construct refer to a representation of a concept *within a given statistical 
    model*. While a concept is defined by a conceptual (theoretical) definition, 
    a construct for a concept is defined/created by the researcher's operationalization
    of a concept within a statistical model. concepts are either modeled
    as common factors/latent variables or as composites. Both operationalizations
    - the common factor and the composite - are called constructs in **cSEM** .
    As opposed to concepts, constructs therefore exist because they arise as 
    the result of the act of modeling their relation to the observable 
    variables (indicators) based on a specific set of assumptions. 
    Constructs may therefore best be understood as stand-ins for concepts. Consequently,
    constructs do not necessarily represent the concept they seek to represent.

Factor-based methods 

:   Factor-based methods or factor-based SEM refers to the entirety of methods 
    centered around the use of the common factors/latent variables as stand-ins for 
    the concepts under investigation. Factor-based methods are also called
    covariance-based methods as focal parameters are retrived such that the
    difference between the model-implied $\bm\Sigma(\theta)$ and the empirical indicator covariance 
    matrix $\bm S$ is minimized.
    
Indicator

:   An observable variable. In **cSEM** observable variables are generally refered 
    to as indicators, however, terms such as item, manifest variable, or 
    observables are sometimes used synommenoulsy.
    
Latent variable

:   See: Common factor

True score

:   See: Common factor
    
Proxy

:   Any quantity that functions as a representation of some other quantity.
    Proxies are usually - but not necessarily - error-prone representations
    of the quantity they seek to represent. Examples are 

Stand-in

:   See: Proxy    

Structural Equation Modeling 

:   The entirety of a set of related theories, mathematical models, methods, 
    algorithms and terminologies related to analyzing the relationships 
    between (unobserved) concepts and/or observables. 
    
## Notation {#notation}
<center>
| Symbol            |  Dimension              |  Description
|:------------------|:-----------------------:|:-------------------------------------|
| $\bm\eta_{en}$	  | $(J_{en} \times 1)$     |  Vector of endogenous constructs/concepts|
| $\bm\eta_{ex}$	  | $(J_{ex} \times 1)$     |  Vector of exogenous constructs/concepts|
| $\bm\eta$         | $(J \times 1)$          |  Vector of constructs/concepts|
| $\eta_j$          | $(1 \times 1)$          |  The $j$'th construct/concept|
</center>

# Literature
